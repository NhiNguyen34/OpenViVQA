{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Khởi tạo tokenizer (bạn cần thay thế bằng tokenizer phù hợp với mô hình BERT của bạn)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tạo dữ liệu mẫu\n",
    "question = \"What color is the car?\"\n",
    "question_tokens = tokenizer(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Số lượng vùng ảnh và token OCR\n",
    "num_regions = 5\n",
    "num_ocr_tokens = 3\n",
    "\n",
    "# Tạo các tensor đặc trưng ngẫu nhiên (thay thế bằng dữ liệu thực tế của bạn)\n",
    "region_features = torch.randn(1, num_regions, 2048)  \n",
    "region_boxes = torch.randn(1, num_regions, 4)\n",
    "ocr_fasttext_features = torch.randn(1, num_ocr_tokens, 300)\n",
    "ocr_rec_features = torch.randn(1, num_ocr_tokens, 256)\n",
    "ocr_det_features = torch.randn(1, num_ocr_tokens, 256)\n",
    "ocr_boxes = torch.randn(1, num_ocr_tokens, 4)\n",
    "\n",
    "# Tạo tensor answer_tokens (chỉ cần thiết cho quá trình huấn luyện)\n",
    "answer_tokens = torch.tensor([[101, 5023, 6012, 102]])  # Ví dụ: [CLS] red [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa lớp MockItems\n",
    "class MockItems:\n",
    "    def __init__(self, question_tokens, region_features, region_boxes,\n",
    "                 ocr_fasttext_features, ocr_rec_features, ocr_det_features, ocr_boxes,\n",
    "                 answer_tokens=None):\n",
    "        self.question_tokens = question_tokens\n",
    "        self.region_features = region_features\n",
    "        self.region_boxes = region_boxes\n",
    "        self.ocr_fasttext_features = ocr_fasttext_features\n",
    "        self.ocr_rec_features = ocr_rec_features\n",
    "        self.ocr_det_features = ocr_det_features\n",
    "        self.ocr_boxes = ocr_boxes\n",
    "        if answer_tokens is not None:\n",
    "            self.answer_tokens = answer_tokens\n",
    "        else:\n",
    "            self.answer_tokens = None  # Để không gây lỗi khi thực hiện inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo đối tượng MockItems\n",
    "items = MockItems(question_tokens, region_features, region_boxes,\n",
    "                 ocr_fasttext_features, ocr_rec_features, ocr_det_features, ocr_boxes,\n",
    "                 answer_tokens)  # Truyền answer_tokens nếu đang huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Các token đặc biệt\n",
    "        self.padding_idx = 0\n",
    "        self.bos_idx = 1  # Bắt đầu câu (Begin of sentence)\n",
    "        self.eos_idx = 2  # Kết thúc câu (End of sentence)\n",
    "        self.unk_idx = 3  # Từ không có trong từ điển (Unknown)\n",
    "\n",
    "        # Từ điển\n",
    "        self.idx2word = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"] + [\"car\", \"red\", \"blue\", \"what\", \"is\", \"the\", \"color\", \"?\"] \n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.idx2word)}\n",
    "\n",
    "        # Độ dài tối đa của câu trả lời\n",
    "        self.max_answer_length = 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "vocab = Vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'TASK': 'TrainingMMF', 'DATASET': CfgNode({'FEATURE_DATASET': CfgNode({'TYPE': 'OcrFeatureDataset', 'BATCH_SIZE': 64, 'WORKERS': 2, 'FEATURE_PATH': CfgNode({'FEATURES': 'features/OpenViVQA/features/x152++_faster_rcnn', 'SCENE_TEXT': 'features/OpenViVQA/features/swintextspotter', 'IMAGE': None}), 'SCENE_TEXT_THRESHOLD': 0.0, 'MAX_SCENE_TEXT': 100, 'WORD_EMBEDDING': 'ViFastText', 'WORD_EMBEDDING_CACHE': None}), 'DICT_DATASET': CfgNode({'TYPE': 'OcrDictionaryDataset', 'BATCH_SIZE': 64, 'WORKERS': 2, 'FEATURE_PATH': CfgNode({'FEATURES': 'features/OpenViVQA/features/x152++_faster_rcnn', 'SCENE_TEXT': 'features/OpenViVQA/features/swintextspotter', 'IMAGE': None}), 'SCENE_TEXT_THRESHOLD': 0.0, 'MAX_SCENE_TEXT': 100, 'WORD_EMBEDDING': 'ViFastText', 'WORD_EMBEDDING_CACHE': None}), 'MIN_FREQ': 1, 'VOCAB': CfgNode({'TYPE': 'OcrVocab', 'TOKENIZER': None, 'WORD_EMBEDDING': None, 'WORD_EMBEDDING_CACHE': None, 'MIN_FREQ': 1, 'BOS_TOKEN': '<bos>', 'EOS_TOKEN': '<eos>', 'PAD_TOKEN': '<pad>', 'UNK_TOKEN': '<unk>', 'IMG_TOKEN': '<img>', 'FEAT_TOKEN': '<feat>', 'BOX_TOKEN': '<box>', 'OCR_TOKEN': '<ocr>', 'OCR_DET_TOKEN': '<ocr_det>', 'OCR_REC_TOKEN': '<ocr_rec>', 'QUESTION_TOKEN': '<question>', 'ANSWER_TOKEN': '<answer>', 'JSON_PATH': CfgNode({'TRAIN': 'features/OpenViVQA/annotations/OpenViVQA_train.json', 'DEV': 'features/OpenViVQA/annotations/OpenViVQA_dev.json', 'TEST': 'features/OpenViVQA/annotations/OpenViVQA_test.json'})}), 'JSON_PATH': CfgNode({'TRAIN': 'features/OpenViVQA/annotations/OpenViVQA_train.json', 'DEV': 'features/OpenViVQA/annotations/OpenViVQA_dev.json', 'TEST': 'features/OpenViVQA/annotations/OpenViVQA_test.json'})}), 'TRAINING': CfgNode({'CHECKPOINT_PATH': 'saved_models', 'LEARNING_RATE': 1.0, 'RL_LEARNING_RATE': 5e-06, 'WARMUP': 10000, 'SCORE': 'CIDEr', 'TRAINING_BEAM_SIZE': 1, 'EVALUATING_BEAM_SIZE': 1, 'PATIENCE': 5}), 'ARCHITECTURE': 'MMF_M4C', 'NAME': 'mmf_m4c_x152++_faster_rcnn', 'DEVICE': 'cuda', 'D_MODEL': 768, 'OBJECT_EMBEDDING': CfgNode({'D_FEATURE': 1024, 'DROPOUT': 0.1}), 'BOX_EMBEDDING': CfgNode({'ARCHITECTURE': 'FeatureEmbedding', 'DEVICE': 'cuda', 'D_FEATURE': 4, 'D_MODEL': 768, 'DROPOUT': 0.1}), 'OCR_TEXT_EMBEDDING': CfgNode({'WORD_EMBEDDING': 'ViFastText', 'WORD_EMBEDDING_CACHE': None}), 'OCR_EMBEDDING': CfgNode({'D_FEATURE': 812, 'DROPOUT': 0.1}), 'OCR_PTR_NET': CfgNode({'HIDDEN_SIZE': 768, 'QUERY_KEY_SIZE': 768, 'OCR_MAX_NUM': 30}), 'CLASSIFIER': CfgNode({'OCR_MAX_NUM': 30}), 'MMT': CfgNode({'HIDDEN_SIZE': 768, 'NUM_HIDDEN_LAYERS': 4, 'NUM_ATTENTION_HEADS': 8}), 'TEXT_BERT': CfgNode({'HIDDEN_SIZE': 768, 'NUM_HIDDEN_LAYERS': 12, 'NUM_ATTENTION_HEADS': 12, 'D_MODEL': 512, 'DROPOUT': 0.1, 'LOAD_PRETRAINED': True, 'FREEZE_WEIGHTS': True, 'PRETRAINED_NAME': 'bert-base-uncased'}), 'TEXT_EMBEDDING': CfgNode({'ARCHITECTURE': 'UsualEmbedding', 'DEVICE': 'cuda', 'D_EMBEDDING': 300, 'D_MODEL': 768, 'DROPOUT': 0.1, 'WORD_EMBEDDING': None, 'WORD_EMBEDDING_CACHE': None})})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configs.utils import get_config\n",
    "\n",
    "config = get_config(r\"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\configs\\mmf_m4c.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'HIDDEN_SIZE': 768, 'NUM_HIDDEN_LAYERS': 4, 'NUM_ATTENTION_HEADS': 8})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.MMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TextBert: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing TextBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TextBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MockItems object at 0x0000024DA2184F88>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25756\\498263371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m# Kiểm tra kết quả\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\models\\mmf_m4c.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_txt_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwd_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_obj_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwd_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_ocr_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwd_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_mmt_and_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwd_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\models\\mmf_m4c.py\u001b[0m in \u001b[0;36m_forward_mmt_and_output\u001b[1;34m(self, items, fwd_results)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mfwd_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"prev_inds\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manswer_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_mmt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwd_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwd_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from models.mmf_m4c import MMF_M4C\n",
    "\n",
    "\n",
    "\n",
    "# Khởi tạo tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = MMF_M4C(config, vocab)\n",
    "\n",
    "# Câu hỏi\n",
    "question = \"What color is the car?\"\n",
    "question_tokens = tokenizer(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Số lượng vùng ảnh và token OCR\n",
    "num_regions = 5\n",
    "num_ocr_tokens = 3\n",
    "\n",
    "# Tạo các tensor đặc trưng ngẫu nhiên (thay thế bằng dữ liệu thực tế của bạn)\n",
    "region_features = torch.randn(1, num_regions, 1024)  \n",
    "region_boxes = torch.randn(1, num_regions, 4)\n",
    "ocr_fasttext_features = torch.randn(1, num_ocr_tokens, 300)\n",
    "ocr_rec_features = torch.randn(1, num_ocr_tokens, 256)\n",
    "ocr_det_features = torch.randn(1, num_ocr_tokens, 256)\n",
    "ocr_boxes = torch.randn(1, num_ocr_tokens, 4)\n",
    "\n",
    "# Tạo tensor answer_tokens (chỉ cần thiết cho quá trình huấn luyện)\n",
    "answer_tokens = torch.tensor([[101, 5023, 6012, 102]])  # Ví dụ: [CLS] red [SEP]\n",
    "\n",
    "\n",
    "# Updated MockItems class with batch_size attribute\n",
    "class MockItems:\n",
    "    def __init__(self, question_tokens, region_features, region_boxes,\n",
    "                 ocr_fasttext_features, ocr_rec_features, ocr_det_features, ocr_boxes,\n",
    "                 answer_tokens=None):\n",
    "        self.question_tokens = question_tokens\n",
    "        self.region_features = region_features\n",
    "        self.region_boxes = region_boxes\n",
    "        self.ocr_fasttext_features = ocr_fasttext_features\n",
    "        self.ocr_rec_features = ocr_rec_features\n",
    "        self.ocr_det_features = ocr_det_features\n",
    "        self.ocr_boxes = ocr_boxes\n",
    "        self.batch_size = question_tokens.size(0)  # Add batch_size attribute\n",
    "        if answer_tokens is not None:\n",
    "            self.answer_tokens = answer_tokens\n",
    "        else:\n",
    "            self.answer_tokens = None  # Để không gây lỗi khi thực hiện inference\n",
    "\n",
    "# Tạo đối tượng MockItems\n",
    "items = MockItems(question_tokens, region_features, region_boxes,\n",
    "                 ocr_fasttext_features, ocr_rec_features, ocr_det_features, ocr_boxes,\n",
    "                 answer_tokens)  \n",
    "\n",
    "\n",
    "# Chuyển mô hình sang chế độ eval (đánh giá)\n",
    "model.eval() \n",
    "\n",
    "# Thực hiện forward pass\n",
    "with torch.no_grad():\n",
    "    print(items)\n",
    "    outputs = model(items)\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "print(outputs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations': [{'question': 'Con mèo đang làm gì?',\n",
       "   'answers': ['Ngủ', 'Nằm ngủ'],\n",
       "   'image_id': 'image1.jpg'},\n",
       "  {'question': 'Màu sắc của quả bóng là gì?',\n",
       "   'answers': ['Đỏ'],\n",
       "   'image_id': 'image2.jpg'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PATH = r'.\\annotations\\OpenViVQA_train.json'\n",
    "with open(PATH, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.jo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6860\\1327429393.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Folder-Anaconda\\envs\\mmf\\lib\\ntpath.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Join two (or more) paths.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb'\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.join(dir.split('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./annotations/OpenViVQA_train.json'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dir = 'features/OpenViVQA/annotations/OpenViVQA_train.json'\n",
    "parts = json_dir.split('/')\n",
    "p =  './' + '/'.join(parts[-2:])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 292: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6860\\3920278391.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjson_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./annotations/OpenViVQA_train.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Folder-Anaconda\\envs\\mmf\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Folder-Anaconda\\envs\\mmf\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 292: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "json_dir = 'features/OpenViVQA/annotations/OpenViVQA_train.json'\n",
    "\n",
    "json_dir = './annotations/OpenViVQA_train.json'\n",
    "json_data = json.load(open(json_dir), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[19/07/2024 16:26:44] INFO: Loading vocab from saved_models\\mmf_m4c_x152++_faster_rcnn\\vocab.bin\u001b[0m\n",
      "\u001b[32m[19/07/2024 16:26:44] INFO: Loading data\u001b[0m\n",
      "{'id': 'image1.jpg', 'filename': 'image1.jpg'}\n",
      "{'id': '1', 'QA-type': '', 'question': 'Con mèo đang làm gì?', 'answers': ['Ngủ', 'Nằm ngủ'], 'image_id': 'image1.jpg'}\n",
      "{'id': 'image1.jpg', 'filename': 'image1.jpg'}\n",
      "{'id': '1', 'QA-type': '', 'question': 'Con mèo đang làm gì?', 'answers': ['Ngủ', 'Nằm ngủ'], 'image_id': 'image1.jpg'}\n",
      "{'id': 'image1.jpg', 'filename': 'image1.jpg'}\n",
      "{'id': '1', 'QA-type': '', 'question': 'Con mèo đang làm gì?', 'answers': ['Ngủ', 'Nằm ngủ'], 'image_id': 'image1.jpg'}\n",
      "{'id': 'image1.jpg', 'filename': 'image1.jpg'}\n",
      "{'id': '1', 'QA-type': '', 'question': 'Con mèo đang làm gì?', 'answers': ['Ngủ', 'Nằm ngủ'], 'image_id': 'image1.jpg'}\n",
      "{'id': 'image1.jpg', 'filename': 'image1.jpg'}\n",
      "{'id': '1', 'QA-type': '', 'question': 'Con mèo đang làm gì?', 'answers': ['Ngủ', 'Nằm ngủ'], 'image_id': 'image1.jpg'}\n",
      "{'id': 'image1.jpg', 'filename': 'image1.jpg'}\n",
      "{'id': '1', 'QA-type': '', 'question': 'Con mèo đang làm gì?', 'answers': ['Ngủ', 'Nằm ngủ'], 'image_id': 'image1.jpg'}\n",
      "\u001b[32m[19/07/2024 16:26:44] INFO: Building model\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TextBert: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TextBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TextBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\train.py\", line 16, in <module>\n",
      "    task = build_task(config)\n",
      "  File \"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\builders\\task_builder.py\", line 6, in build_task\n",
      "    task = META_TASK.get(config.TASK)(config)\n",
      "  File \"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\tasks\\training_mmf_task.py\", line 42, in __init__\n",
      "    super().__init__(config)\n",
      "  File \"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\tasks\\open_ended_task.py\", line 26, in __init__\n",
      "    super().__init__(config)\n",
      "  File \"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\tasks\\base_task.py\", line 40, in __init__\n",
      "    self.model = build_model(config.MODEL, self.vocab)\n",
      "  File \"D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\builders\\model_builder.py\", line 8, in build_model\n",
      "    model = model.to(torch.device(config.DEVICE))\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 907, in to\n",
      "    return self._apply(convert)\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 578, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 578, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 578, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 601, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 905, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 210, in _lazy_init\n",
      "    raise AssertionError(\"Torch not compiled with CUDA enabled\")\n",
      "AssertionError: Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "!python D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\train.py --config D:\\vitextcaps-vietnam-image-captioning-dataset\\OpenViVQA\\configs\\mmf_m4c.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger OpenViVQA (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from configs.utils import get_config\n",
    "from builders.task_builder import build_task\n",
    "from utils.logging_utils import setup_logger\n",
    "\n",
    "logger = setup_logger()\n",
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Folder-Anaconda\\envs\\mmf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6860\\1389277068.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase_task\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseTask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbuilders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_builder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMETA_TASK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbuilders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_builder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from utils.logging_utils import setup_logger\n",
    "from utils.instance import Instance\n",
    "from data_utils.utils import collate_fn\n",
    "from .base_task import BaseTask\n",
    "from builders.task_builder import META_TASK\n",
    "from builders.dataset_builder import build_dataset\n",
    "import evaluation\n",
    "from evaluation import Cider\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from shutil import copyfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        running_loss = .0\n",
    "        with tqdm(desc='Epoch %d - Training with cross-entropy loss' % self.epoch, unit='it', total=len(self.train_dataloader)) as pbar:\n",
    "            for it, items in enumerate(self.train_dataloader):\n",
    "                items = items.to(self.device)\n",
    "                out = self.model(items).contiguous()\n",
    "                shifted_right_answer_tokens = items.shifted_right_answer_tokens\n",
    "                self.optim.zero_grad()\n",
    "                loss = self.loss_fn(out.view(-1, out.shape[-1]), shifted_right_answer_tokens.view(-1))\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                this_loss = loss.item()\n",
    "                running_loss += this_loss\n",
    "\n",
    "                pbar.set_postfix(loss=running_loss / (it + 1))\n",
    "                pbar.update()\n",
    "                self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "@META_TASK.register()\n",
    "class OpenEndedTask(BaseTask):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def load_feature_datasets(self, config):\n",
    "        train_dataset = build_dataset(config.JSON_PATH.TRAIN, self.vocab, config.FEATURE_DATASET)\n",
    "        dev_dataset = build_dataset(config.JSON_PATH.DEV, self.vocab, config.FEATURE_DATASET)\n",
    "        test_dataset = build_dataset(config.JSON_PATH.TEST, self.vocab, config.FEATURE_DATASET)\n",
    "\n",
    "        return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "    def load_dict_datasets(self, config):\n",
    "        train_dataset = build_dataset(config.JSON_PATH.TRAIN, self.vocab, config.DICT_DATASET)\n",
    "        dev_dataset = build_dataset(config.JSON_PATH.DEV, self.vocab, config.DICT_DATASET)\n",
    "        test_dataset = build_dataset(config.JSON_PATH.TEST, self.vocab, config.DICT_DATASET)\n",
    "\n",
    "        return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "    def load_datasets(self, config):\n",
    "        self.train_dataset, self.dev_dataset, self.test_dataset = self.load_feature_datasets(config)\n",
    "        self.train_dict_dataset, self.dev_dict_dataset, self.test_dict_dataset = self.load_dict_datasets(config)\n",
    "\n",
    "    def create_feature_dataloaders(self, config):\n",
    "        # creating iterable-dataset data loader\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=config.DATASET.FEATURE_DATASET.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=config.DATASET.FEATURE_DATASET.WORKERS,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        self.dev_dataloader = DataLoader(\n",
    "            dataset=self.dev_dataset,\n",
    "            batch_size=config.DATASET.FEATURE_DATASET.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=config.DATASET.FEATURE_DATASET.WORKERS,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(\n",
    "            dataset=self.test_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=config.DATASET.FEATURE_DATASET.WORKERS,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def create_dict_dataloaders(self, config):\n",
    "        # creating dictionary iterable-dataset data loader\n",
    "        self.train_dict_dataloader = DataLoader(\n",
    "            dataset=self.train_dict_dataset,\n",
    "            batch_size=config.DATASET.DICT_DATASET.BATCH_SIZE // config.TRAINING.TRAINING_BEAM_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        self.dev_dict_dataloader = DataLoader(\n",
    "            dataset=self.dev_dict_dataset,\n",
    "            batch_size=config.DATASET.DICT_DATASET.BATCH_SIZE // config.TRAINING.EVALUATING_BEAM_SIZE,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        self.test_dict_dataloader = DataLoader(\n",
    "            dataset=self.test_dict_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def create_dataloaders(self, config):\n",
    "        self.create_feature_dataloaders(config)\n",
    "        self.create_dict_dataloaders(config)\n",
    "\n",
    "    def configuring_hyperparameters(self, config):\n",
    "        self.epoch = 0\n",
    "        self.warmup = config.TRAINING.WARMUP\n",
    "        self.score = config.TRAINING.SCORE\n",
    "        self.learning_rate = config.TRAINING.LEARNING_RATE\n",
    "        self.rl_learning_rate = config.TRAINING.RL_LEARNING_RATE\n",
    "        self.training_beam_size = config.TRAINING.TRAINING_BEAM_SIZE\n",
    "        self.evaluating_beam_size = config.TRAINING.EVALUATING_BEAM_SIZE\n",
    "        self.patience = config.TRAINING.PATIENCE\n",
    "        self.train_cider = Cider({f\"{idx}\": answer for idx, answer in enumerate(self.train_dataset.answers)})\n",
    "\n",
    "    def evaluate_loss(self, dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = .0\n",
    "        with tqdm(desc='Epoch %d - Validation' % self.epoch, unit='it', total=len(dataloader)) as pbar:\n",
    "            with torch.no_grad():\n",
    "                for it, items in enumerate(dataloader):\n",
    "                    items = items.to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        out = self.model(items).contiguous()\n",
    "                    \n",
    "                    shifted_right_answer_tokens = items.shifted_right_answer_tokens\n",
    "                    loss = self.loss_fn(out.view(-1, out.shape[-1]), shifted_right_answer_tokens.view(-1))\n",
    "                    this_loss = loss.item()\n",
    "                    running_loss += this_loss\n",
    "\n",
    "                    pbar.set_postfix(loss=running_loss / (it + 1))\n",
    "                    pbar.update()\n",
    "\n",
    "        val_loss = running_loss / len(dataloader)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def evaluate_metrics(self, dataloader):\n",
    "        self.model.eval()\n",
    "        gens = {}\n",
    "        gts = {}\n",
    "        with tqdm(desc='Epoch %d - Evaluation' % self.epoch, unit='it', total=len(dataloader)) as pbar:\n",
    "            for it, items in enumerate(dataloader):\n",
    "                items = items.to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    outs, _ = self.model.beam_search(items, batch_size=items.batch_size, beam_size=self.evaluating_beam_size, out_size=1)\n",
    "\n",
    "                answers_gt = items.answers\n",
    "                answers_gen = self.vocab.decode_answer(outs.contiguous().view(-1, self.vocab.max_answer_length), join_words=False)\n",
    "                for i, (gts_i, gen_i) in enumerate(zip(answers_gt, answers_gen)):\n",
    "                    gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                    gens['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                    gts['%d_%d' % (it, i)] = gts_i\n",
    "                pbar.update()\n",
    "\n",
    "        scores, _ = evaluation.compute_scores(gts, gens)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        running_loss = .0\n",
    "        with tqdm(desc='Epoch %d - Training with cross-entropy loss' % self.epoch, unit='it', total=len(self.train_dataloader)) as pbar:\n",
    "            for it, items in enumerate(self.train_dataloader):\n",
    "                items = items.to(self.device)\n",
    "                out = self.model(items).contiguous()\n",
    "                shifted_right_answer_tokens = items.shifted_right_answer_tokens\n",
    "                self.optim.zero_grad()\n",
    "                loss = self.loss_fn(out.view(-1, out.shape[-1]), shifted_right_answer_tokens.view(-1))\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                this_loss = loss.item()\n",
    "                running_loss += this_loss\n",
    "\n",
    "                pbar.set_postfix(loss=running_loss / (it + 1))\n",
    "                pbar.update()\n",
    "                self.scheduler.step()\n",
    "\n",
    "    # def train_scst(self):\n",
    "    #     # design especially for self-critical sequential learning\n",
    "    #     running_reward = .0\n",
    "    #     running_reward_baseline = .0\n",
    "\n",
    "    #     self.model.train()\n",
    "\n",
    "    #     running_loss = .0\n",
    "    #     with tqdm(desc='Epoch %d - Training with self-critical learning' % self.epoch, unit='it', total=len(self.train_dict_dataloader)) as pbar:\n",
    "    #         for it, items in enumerate(self.train_dict_dataloader):\n",
    "    #             items = items.to(self.device)\n",
    "    #             outs, log_probs = self.model.beam_search(items, batch_size=items.batch_size, \n",
    "    #                                                         beam_size=self.training_beam_size, out_size=self.training_beam_size)\n",
    "                \n",
    "    #             self.optim.zero_grad()\n",
    "\n",
    "    #             # Rewards\n",
    "    #             bs = items.question_tokens.shape[0]\n",
    "    #             answers_gt = items.answers\n",
    "    #             answers_gen = self.vocab.decode_answer(outs.contiguous().view(-1, self.vocab.max_answer_length), join_words=True)\n",
    "    #             answers_gt = list(itertools.chain(*([a, ] * self.training_beam_size for a in answers_gt)))\n",
    "    #             gens = {f\"{idx}\": [answer_gen, ] for idx, answer_gen in enumerate(answers_gen)}\n",
    "    #             gts = {f\"{idx}\": answer_gt for idx, answer_gt in enumerate(answers_gt)}\n",
    "    #             reward = self.train_cider.compute_score(gts, gens)[1].astype(np.float32)\n",
    "    #             reward = torch.from_numpy(reward).to(self.device).view(bs, self.training_beam_size)\n",
    "    #             reward_baseline = torch.mean(reward, dim=-1, keepdim=True)\n",
    "    #             loss = -torch.mean(log_probs, -1) * (reward - reward_baseline)\n",
    "\n",
    "    #             loss = loss.mean()\n",
    "    #             loss.backward()\n",
    "    #             self.optim.step()\n",
    "\n",
    "    #             running_loss += loss.item()\n",
    "    #             running_reward += reward.mean().item()\n",
    "    #             running_reward_baseline += reward_baseline.mean().item()\n",
    "    #             pbar.set_postfix(loss=running_loss / (it + 1), reward=running_reward / (it + 1),\n",
    "    #                             reward_baseline=running_reward_baseline / (it + 1))\n",
    "    #             pbar.update()\n",
    "\n",
    "    def start(self):\n",
    "        if os.path.isfile(os.path.join(self.checkpoint_path, \"last_model.pth\")):\n",
    "            checkpoint = self.load_checkpoint(os.path.join(self.checkpoint_path, \"last_model.pth\"))\n",
    "            # use_rl = checkpoint[\"use_rl\"]\n",
    "            best_val_score = checkpoint[\"best_val_score\"]\n",
    "            patience = checkpoint[\"patience\"]\n",
    "            self.epoch = checkpoint[\"epoch\"] + 1\n",
    "            self.optim.load_state_dict(checkpoint['optimizer'])\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        else:\n",
    "            # use_rl = False\n",
    "            best_val_score = .0\n",
    "            patience = 0\n",
    "\n",
    "        while True:\n",
    "            # if not use_rl:\n",
    "            #     self.train()\n",
    "            # else:\n",
    "            #     self.train_scst()\n",
    "\n",
    "            self.train()\n",
    "\n",
    "            # self.evaluate_loss(self.dev_dataloader)\n",
    "\n",
    "            # val scores\n",
    "            scores = self.evaluate_metrics(self.dev_dict_dataloader)\n",
    "            logger.info(\"Validation scores %s\", scores)\n",
    "            val_score = scores[self.score]\n",
    "\n",
    "            # Prepare for next epoch\n",
    "            best = False\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                patience = 0\n",
    "                best = True\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "            # switch_to_rl = False\n",
    "            exit_train = False\n",
    "\n",
    "            if patience == self.patience:\n",
    "                # if not use_rl:\n",
    "                #     use_rl = True\n",
    "                #     switch_to_rl = True\n",
    "                #     patience = 0\n",
    "                #     self.optim = Adam(self.model.parameters(), lr=self.rl_learning_rate)\n",
    "                #     logger.info(\"Switching to RL\")\n",
    "                # else:\n",
    "                #     logger.info('patience reached.')\n",
    "                #     exit_train = True\n",
    "\n",
    "                logger.info('patience reached.')\n",
    "                exit_train = True\n",
    "\n",
    "            # if switch_to_rl and not best:\n",
    "            #     self.load_checkpoint(os.path.join(self.checkpoint_path, \"best_model.pth\"))\n",
    "\n",
    "            self.save_checkpoint({\n",
    "                'best_val_score': best_val_score,\n",
    "                'patience': patience,\n",
    "                # 'use_rl': use_rl\n",
    "            })\n",
    "\n",
    "            if best:\n",
    "                copyfile(os.path.join(self.checkpoint_path, \"last_model.pth\"), \n",
    "                        os.path.join(self.checkpoint_path, \"best_model.pth\"))\n",
    "\n",
    "            if exit_train:\n",
    "                break\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def get_predictions(self):\n",
    "        if not os.path.isfile(os.path.join(self.checkpoint_path, 'best_model.pth')):\n",
    "            logger.error(\"Prediction require the model must be trained. There is no weights to load for model prediction!\")\n",
    "            raise FileNotFoundError(\"Make sure your checkpoint path is correct or the best_model.pth is available in your checkpoint path\")\n",
    "\n",
    "        self.load_checkpoint(os.path.join(self.checkpoint_path, \"best_model.pth\"))\n",
    "\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        overall_gens = {}\n",
    "        overall_gts = {}\n",
    "        with tqdm(desc='Getting predictions: ', unit='it', total=len(self.test_dict_dataloader)) as pbar:\n",
    "            for it, items in enumerate(self.test_dict_dataloader):\n",
    "                items = items.to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    outs, _ = self.model.beam_search(items, batch_size=items.batch_size, beam_size=self.evaluating_beam_size, out_size=1)\n",
    "\n",
    "                answers_gt = items.answers\n",
    "                answers_gen = self.vocab.decode_answer(outs.contiguous().view(-1, self.vocab.max_answer_length), join_words=False)\n",
    "                gts = {}\n",
    "                gens = {}\n",
    "                for i, (gts_i, gen_i) in enumerate(zip(answers_gt, answers_gen)):\n",
    "                    gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                    gens['%d_%d' % (it, i)] = gen_i\n",
    "                    gts['%d_%d' % (it, i)] = gts_i\n",
    "                    overall_gens['%d_%d' % (it, i)] = [gen_i, ]\n",
    "                    overall_gts['%d_%d' % (it, i)] = gts_i\n",
    "                pbar.update()\n",
    "\n",
    "                results.append({\n",
    "                    \"id\": items.question_id,\n",
    "                    \"image_id\": items.image_id,\n",
    "                    \"filename\": items.filename,\n",
    "                    \"gens\": gens,\n",
    "                    \"gts\": gts\n",
    "                })\n",
    "\n",
    "                pbar.update()\n",
    "\n",
    "        scores, _ = evaluation.compute_scores(overall_gts, overall_gens)\n",
    "        logger.info(\"Evaluation scores on test: %s\", scores)\n",
    "\n",
    "        json.dump({\n",
    "            \"results\": results,\n",
    "            **scores,\n",
    "        }, open(os.path.join(self.checkpoint_path, \"test_results.json\"), \"w+\"), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config-file\", type=str, required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "config = get_config(args.config_file)\n",
    "\n",
    "task = build_task(config)\n",
    "task.start()\n",
    "task.get_predictions()\n",
    "logger.info(\"Task done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
